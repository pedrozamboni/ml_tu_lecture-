{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you wanna make interactive plots in Jupyter notebooks\n",
    "#pip install ipympl\n",
    "### Install if needed: pip install ipympl\n",
    "## after importing matplotlib, run:\n",
    "#%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing libraries and suppressing warnings\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef418d62",
   "metadata": {},
   "source": [
    "Exploring the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining file path and reading data\n",
    "path = Path('C:/Users/mailp/Documents/tu_dresden_lectures/DWD_Phaenologie_Apfel_fruehe_Reife_Bluetebeginn_Climate_2023-2025.csv')\n",
    "### Reading the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "### Displaying information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets check the fist 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting only data for the year 2023 and relevant features for clustering\n",
    "cluster_df = df[df.Year==2023]\n",
    "features = ['rad_global_apr', 'soil_moist_apr', 'precip_apr', 'temp_max_apr', 'temp_min_apr', 'Apfelbluete','Hoehe']\n",
    "\n",
    "X = cluster_df[features].dropna()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978dbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ploting features\n",
    "### Link for shapefiles\n",
    "\"\"\"https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\"\"\"\n",
    "### Load Germany shapefile\n",
    "shp_path = Path('C:/Users/mailp/Documents/tu_dresden_lectures/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp')\n",
    "germany = gpd.read_file(shp_path)\n",
    "germany = germany[germany['NAME'] == 'Germany']\n",
    "\n",
    "# Create GeoDataFrame from df\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df['Lon'], df['Lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Plot each feature as a separate map over Germany with normalized colorbars\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i]\n",
    "    germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "    vmin = gdf[feature].min()\n",
    "    vmax = gdf[feature].max()\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column=feature,\n",
    "        cmap='jet',\n",
    "        markersize=8,\n",
    "        legend=True,\n",
    "        alpha=0.8,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    ax.set_title(feature)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3c22b",
   "metadata": {},
   "source": [
    "Pre-processing data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26551b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scaling data\n",
    "def scale_data(X):\n",
    "    scaler = StandardScaler()\n",
    "    return  scaler.fit_transform(X) \n",
    "\n",
    "### data dimension reduction using PCA\n",
    "def reduce_dimensions(X_scaled, n_components=2, var_threshold=0.85,plot=False):\n",
    "    # First fit PCA with all components\n",
    "    pca = PCA(svd_solver='full')\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Calculate cumulative variance ratio\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Find number of components needed for threshold\n",
    "    n_components_auto = np.argmax(cumsum >= var_threshold) + 1\n",
    "    \n",
    "    # Print variance information\n",
    "    print(f\"Total variance explained by {n_components} components: {cumsum[n_components-1]:.3f}\")\n",
    "    print(f\"Components needed for {var_threshold:.1%} variance: {n_components_auto}\")\n",
    "    \n",
    "    # Create final PCA with specified components\n",
    "    pca = PCA(n_components=n_components, svd_solver='full')\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    if plot:\n",
    "        # Plot variance explained\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(range(1, len(cumsum) + 1), cumsum, 'bo-')\n",
    "        plt.axhline(y=var_threshold, color='r', linestyle='--', label=f'{var_threshold:.1%} threshold')\n",
    "        plt.axvline(x=n_components, color='g', linestyle='--', label=f'Current n_components={n_components}')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Variance Explained')\n",
    "        plt.title('PCA Variance Explained')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    return pca, X_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling the features\n",
    "X_scaled = scale_data(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21344218",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reducing dimensions and plotting variance explained for different n_components\n",
    "for i in range(2,7):\n",
    "    pca, X_pca = reduce_dimensions(X_scaled, n_components=i, var_threshold=0.85,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets use n_components=3 for further analysis\n",
    "pca, X_pca = reduce_dimensions(X_scaled, n_components=3,var_threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative distance functions\n",
    "#from scipy.spatial.distance import euclidean, cosine, cityblock, chebyshev\n",
    "\n",
    "def euclidean_dist(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance using numpy.\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "def manhattan_dist(point1, point2):\n",
    "    \"\"\"Calculate Manhattan (cityblock) distance using numpy.\"\"\"\n",
    "    return np.sum(np.abs(point1 - point2))\n",
    "\n",
    "def cosine_dist(point1, point2):\n",
    "    \"\"\"Calculate Cosine distance using numpy.\"\"\"\n",
    "    dot_product = np.dot(point1, point2)\n",
    "    norms = np.linalg.norm(point1) * np.linalg.norm(point2)\n",
    "    return 1 - dot_product / norms\n",
    "\n",
    "def chebyshev_dist(point1, point2):\n",
    "    \"\"\"Calculate Chebyshev distance using numpy.\"\"\"\n",
    "    return np.max(np.abs(point1 - point2))\n",
    "\n",
    "def chord_dist(point1, point2):\n",
    "    \"\"\"Calculate Chord distance using numpy.\"\"\"\n",
    "    dot_product = np.dot(point1, point2)\n",
    "    norms = np.linalg.norm(point1) * np.linalg.norm(point2)\n",
    "    return np.sqrt(2 * (1 - dot_product / norms))\n",
    "\n",
    "# Update the plot_distances function to use these functions\n",
    "def plot_distances(point1, point2):\n",
    "    # Calculate distances using the new functions\n",
    "    metrics = [\n",
    "        ('Euclidean', euclidean_dist(point1, point2)),\n",
    "        ('Manhattan', manhattan_dist(point1, point2)),\n",
    "        ('Cosine', cosine_dist(point1, point2)),\n",
    "        ('Chebyshev', chebyshev_dist(point1, point2)),\n",
    "    ]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot points in each subplot\n",
    "    for i, (metric_name, value) in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot points\n",
    "        ax.scatter([point1[0], point2[0]], [point1[1], point2[1]], c='blue')\n",
    "        \n",
    "        # Annotate points\n",
    "        ax.annotate('Point 1', (point1[0], point1[1]), xytext=(10, 10), \n",
    "                   textcoords='offset points')\n",
    "        ax.annotate('Point 2', (point2[0], point2[1]), xytext=(10, 10), \n",
    "                   textcoords='offset points')\n",
    "        \n",
    "        # Visualize distance calculation\n",
    "        if metric_name == 'Euclidean':\n",
    "            # Draw direct line\n",
    "            ax.plot([point1[0], point2[0]], [point1[1], point2[1]], 'r--')\n",
    "            \n",
    "        elif metric_name == 'Manhattan':\n",
    "            # Draw Manhattan path\n",
    "            ax.plot([point1[0], point2[0]], [point1[1], point1[1]], 'r--')\n",
    "            ax.plot([point2[0], point2[0]], [point1[1], point2[1]], 'r--')\n",
    "            \n",
    "        elif metric_name == 'Chebyshev':\n",
    "            # Draw box showing maximum distance\n",
    "            width = abs(point2[0] - point1[0])\n",
    "            height = abs(point2[1] - point1[1])\n",
    "            # ax.add_patch(plt.Rectangle((min(point1[0], point2[0]), \n",
    "            #                           min(point1[1], point2[1])), \n",
    "            #                          width, height, fill=False, \n",
    "            #                          linestyle='--', color='red'))\n",
    "            if width > height:\n",
    "                # project point2 onto horizontal line through point1 and draw horizontal connector\n",
    "                proj = np.array([point2[0], point1[1]])\n",
    "                # dominant horizontal segment (illustrates Chebyshev = width)\n",
    "                ax.plot([point1[0], proj[0]], [point1[1], proj[1]], 'r--')\n",
    "                # faint vertical connector from projection to the actual second point\n",
    "                ax.plot([proj[0], point2[0]], [proj[1], point2[1]], 'r--', alpha=0.5)\n",
    "            else:\n",
    "                # project point2 onto vertical line through point1 and draw vertical connector\n",
    "                proj = np.array([point1[0], point2[1]])\n",
    "                # dominant vertical segment (illustrates Chebyshev = height)\n",
    "                ax.plot([point1[0], proj[0]], [point1[1], proj[1]], 'r--')\n",
    "                # faint horizontal connector from projection to the actual second point\n",
    "                ax.plot([proj[0], point2[0]], [proj[1], point2[1]], 'r--', alpha=0.5)\n",
    "\n",
    "            # highlight the projection\n",
    "            ax.scatter([proj[0]], [proj[1]], c='red', zorder=5)\n",
    "            ax.annotate('Projection', (proj[0], proj[1]), xytext=(5, -10),\n",
    "                        textcoords='offset points', color='red')\n",
    "            \n",
    "        elif metric_name in ['Cosine']:\n",
    "            # Draw vectors from origin\n",
    "            ax.plot([0, point1[0]], [0, point1[1]], 'r--')\n",
    "            ax.plot([0, point2[0]], [0, point2[1]], 'r--')\n",
    "            # Draw arc between vectors for Chord distance\n",
    "           \n",
    "        ax.set_title(f'{metric_name}\\nDistance: {value:.3f}')\n",
    "        ax.grid(True)\n",
    "        ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    # Remove extra subplot\n",
    "    axes[-1].remove()\n",
    "    axes[-2].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Take two random points from PCA data (first two components)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "idx1, idx2 = np.random.randint(0, len(X_pca), 2)\n",
    "point1 = X_pca[idx1, :2]\n",
    "point2 = X_pca[idx2, :2]\n",
    "\n",
    "# Plot distances\n",
    "plot_distances(point1, point2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91633d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "def plot_pca_scatter(X_pca, pca, features, color_labels=None,\n",
    "                     figsize=(8,6), elev=30, azim=120,\n",
    "                     s=35, alpha=0.8, cmap='viridis'):\n",
    "   \n",
    "    n_comp = X_pca.shape[1]\n",
    "    \n",
    "    # scale factor for arrows so they’re visible but not huge\n",
    "    arrow_scale = np.max(np.abs(X_pca)) * 1.5\n",
    "\n",
    "    if n_comp == 2:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Scatter points\n",
    "        sc = ax.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                        c=color_labels, cmap=cmap, s=s, alpha=alpha, edgecolor='k')\n",
    "\n",
    "        # Feature loadings (arrows)\n",
    "        for i, feature in enumerate(features):\n",
    "            ax.arrow(0, 0,\n",
    "                     pca.components_[0, i]*arrow_scale,\n",
    "                     pca.components_[1, i]*arrow_scale,\n",
    "                     color='r', alpha=0.6, head_width=0.05)\n",
    "            ax.text(pca.components_[0, i]*arrow_scale*1.15,\n",
    "                    pca.components_[1, i]*arrow_scale*1.15,\n",
    "                    feature, color='r', fontsize=10, ha='center')\n",
    "\n",
    "        ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "        ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "        ax.set_title(\"PCA Biplot (2D)\")\n",
    "\n",
    "        if color_labels is not None:\n",
    "            plt.colorbar(sc, ax=ax, label='Cluster/Label')\n",
    "\n",
    "        ax.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    elif n_comp == 3:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        sc = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                        c=color_labels, cmap=cmap, s=s, alpha=alpha)\n",
    "\n",
    "        # 3D arrows (feature loadings)\n",
    "        for i, feature in enumerate(features):\n",
    "            ax.quiver(0, 0, 0,\n",
    "                      pca.components_[0, i]*arrow_scale,\n",
    "                      pca.components_[1, i]*arrow_scale,\n",
    "                      pca.components_[2, i]*arrow_scale,\n",
    "                      color='r', alpha=0.6)\n",
    "            ax.text(pca.components_[0, i]*arrow_scale,\n",
    "                    pca.components_[1, i]*arrow_scale,\n",
    "                    pca.components_[2, i]*arrow_scale,\n",
    "                feature, color='r', fontsize=10, ha='center',va='center')\n",
    "\n",
    "        ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "        ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "        ax.set_zlabel(f\"PC3 ({pca.explained_variance_ratio_[2]*100:.1f}% var)\")\n",
    "        ax.set_title(\"PCA Biplot (3D)\")\n",
    "        ax.view_init(elev=elev, azim=azim)\n",
    "#\n",
    "        if color_labels is not None:\n",
    "            fig.colorbar(sc, ax=ax, label='Cluster/Label', pad=0.1)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"PCA must have 2 or 3 components, got {n_comp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets plot PCA scatter\n",
    "plot_pca_scatter(X_pca,pca, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5fc848",
   "metadata": {},
   "source": [
    "Starting with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca91f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_kmeans(X, k_min=2, k_max=10, random_state=0):\n",
    "    results = []\n",
    "    for k in range(k_min, k_max+1):\n",
    "        km = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "        labels = km.fit_predict(X)\n",
    "        # silhouette requires at least 2 clusters and less than n_samples clusters\n",
    "        try:\n",
    "            sil = silhouette_score(X, labels)\n",
    "        except Exception:\n",
    "            sil = np.nan\n",
    "        try:\n",
    "            db = davies_bouldin_score(X, labels)\n",
    "        except Exception:\n",
    "            db = np.nan\n",
    "        results.append({\"k\": k, \"inertia\": km.inertia_, \"silhouette\": sil, \"davies_bouldin\": db})\n",
    "    res_df = pd.DataFrame(results)\n",
    "    # choose best k by highest silhouette_score (fall back to lowest davies_bouldin)\n",
    "    if res_df[\"silhouette\"].notna().any():\n",
    "        best_row = res_df.loc[res_df[\"silhouette\"].idxmax()]\n",
    "        selection_reason = \"silhouette\"\n",
    "    else:\n",
    "        best_row = res_df.loc[res_df[\"davies_bouldin\"].idxmin()]\n",
    "        selection_reason = \"davies_bouldin\"\n",
    "    return res_df, int(best_row[\"k\"]), selection_reason\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "def bootstrap_cluster_stability(X, orig_labels, n_clusters, n_bootstrap=100, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n = X.shape[0]\n",
    "    ari_scores = []\n",
    "    unique_orig = np.unique(orig_labels)\n",
    "    jaccard_per_cluster = {c: [] for c in unique_orig}\n",
    "\n",
    "    for b in range(n_bootstrap):\n",
    "        # bootstrap sample indices (with replacement)\n",
    "        boot_idx = rng.choice(n, size=n, replace=True)\n",
    "        # fit KMeans on bootstrap sample\n",
    "        km = KMeans(n_clusters=n_clusters, n_init=10, random_state=random_state + b)\n",
    "        km.fit(X[boot_idx])\n",
    "        # predict labels for the full dataset using bootstrap-fitted centers\n",
    "        pred_labels = km.predict(X)\n",
    "\n",
    "        # overall stability: Adjusted Rand Index\n",
    "        ari = adjusted_rand_score(orig_labels, pred_labels)\n",
    "        ari_scores.append(ari)\n",
    "\n",
    "        # cluster-wise stability: Jaccard after matching predicted labels to original clusters\n",
    "        cm = contingency_matrix(orig_labels, pred_labels)  # rows: orig clusters, cols: predicted clusters\n",
    "        for i, orig_c in enumerate(unique_orig):\n",
    "            row = cm[i, :]\n",
    "            if row.sum() == 0:\n",
    "                jaccard_per_cluster[orig_c].append(0.0)\n",
    "                continue\n",
    "            # match predicted label with maximum overlap for this original cluster\n",
    "            j = row.argmax()\n",
    "            inter = row[j]\n",
    "            size_orig = row.sum()\n",
    "            size_pred = cm[:, j].sum()\n",
    "            union = size_orig + size_pred - inter\n",
    "            jacc = inter / union if union > 0 else 0.0\n",
    "            jaccard_per_cluster[orig_c].append(jacc)\n",
    "\n",
    "    # summarize results\n",
    "    summary = {\n",
    "        \"n_bootstrap\": n_bootstrap,\n",
    "        \"ari_mean\": float(np.mean(ari_scores)),\n",
    "        \"ari_std\": float(np.std(ari_scores)),\n",
    "        \"ari_scores\": ari_scores,       \n",
    "        \"per_cluster_jaccard_mean\": {int(k): float(np.mean(v)) for k, v in jaccard_per_cluster.items()},\n",
    "        \"per_cluster_jaccard_std\": {int(k): float(np.std(v)) for k, v in jaccard_per_cluster.items()}\n",
    "    }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_kmeans, best_k_kmeans, reason_kmeans = find_best_k_kmeans(X_pca, 2, 10)\n",
    "print(f\"Best k by {reason_kmeans}: {best_k_kmeans}\")\n",
    "results_df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# # Assign clusters back to the DataFrame (NaNs will be dropped!)\n",
    "df_clustered = cluster_df.copy()\n",
    "kmeans_labels = kmeans.fit_predict(X_pca)\n",
    "df_clustered['cluster'] = kmeans.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bootstrap stability check on KMeans result (uses X_pca and km_labels already computed)\n",
    "stability = bootstrap_cluster_stability(X_pca, kmeans_labels, n_clusters=2, n_bootstrap=50, random_state=9)\n",
    "\n",
    "print(\"Bootstrap stability (KMeans):\")\n",
    "print(f\"  Bootstraps: {stability['n_bootstrap']}\")\n",
    "print(f\"  ARI mean ± std: {stability['ari_mean']:.4f} ± {stability['ari_std']:.4f}\")\n",
    "print(\"  Per-cluster Jaccard mean ± std:\")\n",
    "for c in sorted(stability['per_cluster_jaccard_mean'].keys()):\n",
    "    jm = stability['per_cluster_jaccard_mean'][c]\n",
    "    js = stability['per_cluster_jaccard_std'][c]\n",
    "    print(f\"    Cluster {c}: {jm:.3f} ± {js:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11515d",
   "metadata": {},
   "source": [
    "Now, HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'min_cluster_size': [5, 10, 15, 20, 30, 50, 70,100],\n",
    "    'min_samples': [5, 10, 15, 20, 30,50],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Grid search\n",
    "for min_cluster_size in param_grid['min_cluster_size']:\n",
    "    for min_samples in param_grid['min_samples']:\n",
    "        for algorithm in param_grid['algorithm']:\n",
    "            for metric in param_grid['metric']:\n",
    "                try:\n",
    "                    # Initialize and fit HDBSCAN\n",
    "                    clusterer = HDBSCAN(\n",
    "                        min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        algorithm=algorithm,\n",
    "                        metric=metric\n",
    "                    )\n",
    "                    labels = clusterer.fit_predict(X_pca)\n",
    "                    \n",
    "                    # Only evaluate if we have at least 2 clusters (excluding noise)\n",
    "                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                    if n_clusters >= 2:\n",
    "                        # Calculate scores\n",
    "                        sil_score = silhouette_score(X_pca, labels)\n",
    "                        noise_ratio = (labels == -1).sum() / len(labels)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'min_cluster_size': min_cluster_size,\n",
    "                            'min_samples': min_samples,\n",
    "                            'algorithm': algorithm,\n",
    "                            'metric': metric,\n",
    "                            'n_clusters': n_clusters,\n",
    "                            'silhouette': sil_score,\n",
    "                            'noise_ratio': noise_ratio\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find best parameters\n",
    "best_result = results_df.sort_values('silhouette', ascending=False).iloc[0]\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"min_cluster_size: {best_result['min_cluster_size']}\")\n",
    "print(f\"min_samples: {best_result['min_samples']}\")\n",
    "print(f\"algorithm: {best_result['algorithm']}\")\n",
    "print(f\"metric: {best_result['metric']}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Number of clusters: {best_result['n_clusters']}\")\n",
    "print(f\"Silhouette score: {best_result['silhouette']:.3f}\")\n",
    "print(f\"Noise ratio: {best_result['noise_ratio']:.2%}\")\n",
    "\n",
    "# Plot top 10 configurations\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10 = results_df.nlargest(10, 'silhouette')\n",
    "plt.bar(range(10), top_10['silhouette'])\n",
    "plt.xlabel('Configuration rank')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Top 10 HDBSCAN configurations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca67c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = HDBSCAN(min_cluster_size=50,\n",
    "                    min_samples=5,\n",
    "                    metric= 'cosine',\n",
    "                    algorithm='brute')\n",
    "hdb.fit(X_pca)\n",
    "\n",
    "df_clustered_h = cluster_df.copy()\n",
    "\n",
    "df_clustered_h['cluster'] = hdb.labels_\n",
    "\n",
    "final_hdbscan = df_clustered_h[df_clustered_h['cluster'] != -1]  # Remove noise points if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_hdbscan_stability(X, orig_labels, n_bootstrap=100, random_state=0, **hdbscan_params):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n = X.shape[0]\n",
    "    ari_scores = []\n",
    "    unique_orig = np.unique(orig_labels[orig_labels != -1])  # Exclude noise points\n",
    "    jaccard_per_cluster = {c: [] for c in unique_orig}\n",
    "\n",
    "    for b in range(n_bootstrap):\n",
    "        # bootstrap sample indices (with replacement)\n",
    "        boot_idx = rng.choice(n, size=n, replace=True)\n",
    "        \n",
    "        # fit HDBSCAN on bootstrap sample\n",
    "        clusterer = HDBSCAN(**hdbscan_params)\n",
    "        boot_labels = clusterer.fit_predict(X[boot_idx])\n",
    "        \n",
    "        # predict labels for the full dataset\n",
    "        pred_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # Calculate ARI only for non-noise points\n",
    "        mask_orig = orig_labels != -1\n",
    "        mask_pred = pred_labels != -1\n",
    "        if np.any(mask_orig) and np.any(mask_pred):\n",
    "            ari = adjusted_rand_score(orig_labels[mask_orig], pred_labels[mask_orig])\n",
    "            ari_scores.append(ari)\n",
    "\n",
    "        # cluster-wise stability using Jaccard\n",
    "        cm = contingency_matrix(orig_labels, pred_labels)\n",
    "        for i, orig_c in enumerate(unique_orig):\n",
    "            row = cm[int(orig_c), :]\n",
    "            if row.sum() == 0:\n",
    "                jaccard_per_cluster[orig_c].append(0.0)\n",
    "                continue\n",
    "            j = row.argmax()\n",
    "            inter = row[j]\n",
    "            size_orig = row.sum()\n",
    "            size_pred = cm[:, j].sum()\n",
    "            union = size_orig + size_pred - inter\n",
    "            jacc = inter / union if union > 0 else 0.0\n",
    "            jaccard_per_cluster[orig_c].append(jacc)\n",
    "\n",
    "    # summarize results\n",
    "    summary = {\n",
    "        \"n_bootstrap\": n_bootstrap,\n",
    "        \"ari_mean\": float(np.mean(ari_scores)) if ari_scores else 0.0,\n",
    "        \"ari_std\": float(np.std(ari_scores)) if ari_scores else 0.0,\n",
    "        \"per_cluster_jaccard_mean\": {int(k): float(np.mean(v)) for k, v in jaccard_per_cluster.items()},\n",
    "        \"per_cluster_jaccard_std\": {int(k): float(np.std(v)) for k, v in jaccard_per_cluster.items()},\n",
    "        \"noise_ratio\": float(np.mean(orig_labels == -1))\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "# Run stability analysis with the best HDBSCAN parameters\n",
    "stability = bootstrap_hdbscan_stability(\n",
    "    X_pca, \n",
    "    hdb.labels_, \n",
    "    n_bootstrap=50, \n",
    "    random_state=485,\n",
    "    **hdb.get_params()\n",
    ")\n",
    "\n",
    "print(\"Bootstrap stability (HDBSCAN):\")\n",
    "print(f\"  Bootstraps: {stability['n_bootstrap']}\")\n",
    "print(f\"  ARI mean ± std: {stability['ari_mean']:.4f} ± {stability['ari_std']:.4f}\")\n",
    "print(f\"  Noise ratio: {stability['noise_ratio']:.2%}\")\n",
    "print(\"  Per-cluster Jaccard mean ± std:\")\n",
    "for c in sorted(stability['per_cluster_jaccard_mean'].keys()):\n",
    "    jm = stability['per_cluster_jaccard_mean'][c]\n",
    "    js = stability['per_cluster_jaccard_std'][c]\n",
    "    print(f\"    Cluster {c}: {jm:.3f} ± {js:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68874e",
   "metadata": {},
   "source": [
    "Lets see now the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_scatter(X_pca,pca, features, color_labels=kmeans_labels,cmap='jet')\n",
    "plot_pca_scatter(X_pca,pca, features, color_labels=hdb.labels_,cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_scatter(X_pca[:,0:2],pca, features, color_labels=kmeans_labels,cmap='jet')\n",
    "plot_pca_scatter(X_pca[:,0:2],pca, features, color_labels=hdb.labels_,cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "from itertools import cycle\n",
    "from matplotlib import cm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create DataFrame with PCA components and cluster labels\n",
    "df_cluster_all = pd.DataFrame({\n",
    "    'pca1': X_pca[:, 0],\n",
    "    'pca2': X_pca[:, 1], \n",
    "    'pca3': X_pca[:, 2],\n",
    "    'kmeans_label': kmeans_labels,\n",
    "    'hdbscan_label': hdb.labels_\n",
    "})\n",
    "\n",
    "# Remove rows where hdbscan_label is -1\n",
    "df_cluster_all = df_cluster_all[df_cluster_all.hdbscan_label != -1]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "cluster_methods = ['kmeans_label', 'hdbscan_label']\n",
    "titles = ['KMeans Clusters', 'HDBSCAN Clusters']\n",
    "\n",
    "for ax, label_col, title in zip(axes, cluster_methods, titles):\n",
    "    X = df_cluster_all[['pca1', 'pca2']].values\n",
    "    y = df_cluster_all[label_col].values\n",
    "\n",
    "    # Fit KNN classifier to approximate decision boundaries\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Create a grid to plot decision boundaries\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                         np.linspace(y_min, y_max, 500))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = clf.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    # Plot decision regions\n",
    "    unique_labels = np.unique(y)\n",
    "    cmap = cm.get_cmap('jet', len(unique_labels))\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, levels=len(unique_labels)-1, colors=[cmap(i) for i in range(len(unique_labels))])\n",
    "\n",
    "    # Plot points\n",
    "    for i, cluster in enumerate(unique_labels):\n",
    "        pts = X[y == cluster]\n",
    "        ax.scatter(pts[:, 0], pts[:, 1], color=cmap(i), edgecolor='k', s=40, label=f'Cluster {cluster}')\n",
    "\n",
    "    ax.set_xlabel('PCA1')\n",
    "    ax.set_ylabel('PCA2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets check thel PCA loadings\n",
    "pca_df = pd.DataFrame(pca.components_, columns=features, index=[f'PC{i+1}' for i in range(pca.n_components_)]).T\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d30bb6",
   "metadata": {},
   "source": [
    "PC1 - Hoehe and soil moisture (+) and temp min, temp max, radi (-) ---> Climate and altitude gradient\n",
    "\n",
    "PC2 - precipitation, temp mim, temp max (+) and Apfeldbluete and rad (-) ---> precipitation balance and climate\n",
    "\n",
    "PC3 - rad (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median per cluster with features on x-axis and clusters side-by-side\n",
    "medians = df_clustered[features + ['cluster']].groupby('cluster').median()\n",
    "medians = medians[features]  # ensure feature order\n",
    "\n",
    "medians_T = medians.T  # features as rows -> x-axis\n",
    "\n",
    "ax = medians_T.plot(kind='bar', figsize=(12,6))\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Median value')\n",
    "ax.set_title('Median feature values per cluster')\n",
    "ax.legend(title='Cluster', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47588bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median per cluster with features on x-axis and clusters side-by-side\n",
    "medians = df_clustered_h[features + ['cluster']].groupby('cluster').median()\n",
    "medians = medians[features]  # ensure feature order\n",
    "\n",
    "medians_T = medians.T  # features as rows -> x-axis\n",
    "\n",
    "ax = medians_T.plot(kind='bar', figsize=(12,6))\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Median value')\n",
    "ax.set_title('Median feature values per cluster')\n",
    "ax.legend(title='Cluster', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462b294",
   "metadata": {},
   "source": [
    "Lets finally see the cluster on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_map_germany(df_clustered, germany, ax=None):\n",
    "    gdf_points = gpd.GeoDataFrame(\n",
    "        df_clustered,\n",
    "        geometry=gpd.points_from_xy(df_clustered[\"Lon\"], df_clustered[\"Lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Plot base map\n",
    "    germany.plot(ax=ax, color=\"whitesmoke\", alpha=0.6, zorder=1)\n",
    "    germany.boundary.plot(ax=ax, linewidth=1, color='black', zorder=2)\n",
    "\n",
    "    # Plot points\n",
    "    gdf_points.plot(\n",
    "        ax=ax,\n",
    "        column=\"cluster\",\n",
    "        categorical=True,\n",
    "        legend=True,\n",
    "        cmap='jet',  # Using a colormap that works well with categorical data\n",
    "        markersize=15,\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    return ax\n",
    "\n",
    "# Create figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot KMeans clusters\n",
    "plot_cluster_map_germany(df_clustered, germany, ax=ax1)\n",
    "ax1.set_title(\"KMeans Clusters\", pad=10, fontsize=12)\n",
    "\n",
    "# Plot HDBSCAN clusters\n",
    "plot_cluster_map_germany(final_hdbscan, germany, ax=ax2)\n",
    "ax2.set_title(\"HDBSCAN Clusters\", pad=10, fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each feature as a separate map over Germany with normalized colorbars\n",
    "features_to_plot = ['rad_global_apr', 'soil_moist_apr', 'precip_apr', 'temp_max_apr', 'temp_min_apr', 'Apfelbluete','Hoehe']\n",
    "\n",
    "# Create a larger figure with 4 rows and 3 columns\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    ax = axes[i]\n",
    "    germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "    vmin = gdf[gdf.Year==2023][feature].min()\n",
    "    vmax = gdf[gdf.Year==2023][feature].max()\n",
    "    gdf[gdf.Year==2023].plot(\n",
    "        ax=ax,\n",
    "        column=feature,\n",
    "        cmap='jet',\n",
    "        markersize=20,\n",
    "        legend=True,\n",
    "        alpha=0.8,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    ax.set_title(feature)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# Plot KMeans clusters in the 8th subplot\n",
    "ax = axes[7]\n",
    "germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_kmeans = gpd.GeoDataFrame(\n",
    "    df_clustered,\n",
    "    geometry=gpd.points_from_xy(df_clustered['Lon'], df_clustered['Lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_kmeans.plot(\n",
    "    ax=ax,\n",
    "    column='cluster',\n",
    "    categorical=True,\n",
    "    cmap='jet',\n",
    "    markersize=20,\n",
    "    legend=True,\n",
    "    alpha=0.8\n",
    ")\n",
    "ax.set_title('KMeans Clusters')\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Plot HDBSCAN clusters in the 9th subplot\n",
    "ax = axes[8]\n",
    "germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_hdbscan = gpd.GeoDataFrame(\n",
    "    final_hdbscan,\n",
    "    geometry=gpd.points_from_xy(final_hdbscan['Lon'], final_hdbscan['Lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_hdbscan.plot(\n",
    "    ax=ax,\n",
    "    column='cluster',\n",
    "    categorical=True,\n",
    "    cmap='jet',\n",
    "    markersize=20,\n",
    "    legend=True,\n",
    "    alpha=0.8\n",
    ")\n",
    "ax.set_title('HDBSCAN Clusters')\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add PCA components to the plot\n",
    "# Create a GeoDataFrame with PCA components\n",
    "gdf_pca = gpd.GeoDataFrame(\n",
    "    df_clustered.copy(),\n",
    "    geometry=gpd.points_from_xy(df_clustered['Lon'], df_clustered['Lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_pca['PC1'] = X_pca[:, 0]\n",
    "gdf_pca['PC2'] = X_pca[:, 1]\n",
    "gdf_pca['PC3'] = X_pca[:, 2]\n",
    "\n",
    "# Plot PC1\n",
    "ax = axes[9]\n",
    "germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_pca.plot(\n",
    "    ax=ax,\n",
    "    column='PC1',\n",
    "    cmap='jet',\n",
    "    markersize=20,\n",
    "    legend=True,\n",
    "    alpha=0.8,\n",
    "    vmin=gdf_pca['PC1'].min(),\n",
    "    vmax=gdf_pca['PC1'].max()\n",
    ")\n",
    "ax.set_title('PC1 (Principal Component 1)')\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Plot PC2\n",
    "ax = axes[10]\n",
    "germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_pca.plot(\n",
    "    ax=ax,\n",
    "    column='PC2',\n",
    "    cmap='jet',\n",
    "    markersize=20,\n",
    "    legend=True,\n",
    "    alpha=0.8,\n",
    "    vmin=gdf_pca['PC2'].min(),\n",
    "    vmax=gdf_pca['PC2'].max()\n",
    ")\n",
    "ax.set_title('PC2 (Principal Component 2)')\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Plot PC3\n",
    "ax = axes[11]\n",
    "germany.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_pca.plot(\n",
    "    ax=ax,\n",
    "    column='PC3',\n",
    "    cmap='jet',\n",
    "    markersize=20,\n",
    "    legend=True,\n",
    "    alpha=0.8,\n",
    "    vmin=gdf_pca['PC3'].min(),\n",
    "    vmax=gdf_pca['PC3'].max()\n",
    ")\n",
    "ax.set_title('PC3 (Principal Component 3)')\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1a417",
   "metadata": {},
   "source": [
    "Let`s go do some other clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d3aaf3",
   "metadata": {},
   "source": [
    "Mean shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "quantiles = [ 0.05,0.08, 0.1, 0.2, 0.3,0.5,1]\n",
    "\n",
    "for q in quantiles:\n",
    "    # estimate bandwidth and fit MeanShift on PCA data\n",
    "    bw = cluster.estimate_bandwidth(X_pca, quantile=q)\n",
    "    ms = cluster.MeanShift(bandwidth=bw, bin_seeding=True)\n",
    "    ms.fit(X_pca)\n",
    "    labels_ms = ms.labels_\n",
    "    n_clusters = len(np.unique(labels_ms))\n",
    "    print(f\"quantile={q}  bandwidth={bw:.4f}  n_clusters={n_clusters}\")\n",
    "\n",
    "    # PCA scatter (3D if PCA has 3 components)\n",
    "    plot_pca_scatter(X_pca, pca, features, color_labels=labels_ms, cmap='jet')\n",
    "\n",
    "    # Map view for Germany\n",
    "    df_ms = cluster_df.copy()\n",
    "    df_ms['cluster'] = labels_ms\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n",
    "    plot_cluster_map_germany(df_ms, germany, ax=ax)\n",
    "    ax.set_title(f\"MeanShift clusters (quantile={q}, bw={bw:.3f})\", pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75725",
   "metadata": {},
   "source": [
    "Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de553df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn import cluster as skcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "n_list = [2, 3, 4]\n",
    "\n",
    "for n_clusters in n_list:\n",
    "    print(f\"\\n--- n_clusters = {n_clusters} ---\")\n",
    "    # --- Gaussian Mixture ---\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=n_clusters,\n",
    "        covariance_type=\"full\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    labels_gmm = gmm.fit_predict(X_pca)\n",
    "    try:\n",
    "        sil_gmm = silhouette_score(X_pca, labels_gmm) if len(np.unique(labels_gmm)) > 1 else np.nan\n",
    "    except Exception:\n",
    "        sil_gmm = np.nan\n",
    "    print(f\"GMM: n_clusters={len(np.unique(labels_gmm))}, silhouette={sil_gmm:.3f}\")\n",
    "\n",
    "    # --- Spectral Clustering ---\n",
    "    spectral = skcluster.SpectralClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    try:\n",
    "        labels_spectral = spectral.fit_predict(X_pca)\n",
    "        sil_spec = silhouette_score(X_pca, labels_spectral) if len(np.unique(labels_spectral)) > 1 else np.nan\n",
    "    except Exception as e:\n",
    "        labels_spectral = np.full(len(X_pca), -1)\n",
    "        sil_spec = np.nan\n",
    "        print(f\"Spectral failed for n={n_clusters}: {e}\")\n",
    "    print(f\"Spectral: n_clusters={len(np.unique(labels_spectral))}, silhouette={sil_spec:.3f}\")\n",
    "\n",
    "    # --- 3D side-by-side PCA plots ---\n",
    "    if X_pca.shape[1] < 3:\n",
    "        print(\"PCA has less than 3 components, skipping 3D plots.\")\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "        # GMM subplot\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        sc1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                          c=labels_gmm, cmap='jet', s=35, alpha=0.8)\n",
    "        arrow_scale = np.max(np.abs(X_pca)) * 1.2\n",
    "        for i, feature in enumerate(features):\n",
    "            ax1.quiver(0, 0, 0,\n",
    "                       pca.components_[0, i] * arrow_scale,\n",
    "                       pca.components_[1, i] * arrow_scale,\n",
    "                       pca.components_[2, i] * arrow_scale,\n",
    "                       color='r', alpha=0.6, linewidth=1)\n",
    "            ax1.text(pca.components_[0, i] * arrow_scale,\n",
    "                     pca.components_[1, i] * arrow_scale,\n",
    "                     pca.components_[2, i] * arrow_scale,\n",
    "                     feature, color='r', fontsize=9, ha='center', va='center')\n",
    "        ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "        ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "        ax1.set_zlabel(f\"PC3 ({pca.explained_variance_ratio_[2]*100:.1f}% var)\")\n",
    "        ax1.view_init(elev=30, azim=120)\n",
    "        ax1.set_title(f\"GMM clusters (n={n_clusters})\\nSilhouette={sil_gmm:.3f}\")\n",
    "        fig.colorbar(sc1, ax=ax1, pad=0.05, shrink=0.6)\n",
    "\n",
    "        # Spectral subplot\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        sc2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                          c=labels_spectral, cmap='jet', s=35, alpha=0.8)\n",
    "        for i, feature in enumerate(features):\n",
    "            ax2.quiver(0, 0, 0,\n",
    "                       pca.components_[0, i] * arrow_scale,\n",
    "                       pca.components_[1, i] * arrow_scale,\n",
    "                       pca.components_[2, i] * arrow_scale,\n",
    "                       color='r', alpha=0.6, linewidth=1)\n",
    "            ax2.text(pca.components_[0, i] * arrow_scale,\n",
    "                     pca.components_[1, i] * arrow_scale,\n",
    "                     pca.components_[2, i] * arrow_scale,\n",
    "                     feature, color='r', fontsize=9, ha='center', va='center')\n",
    "        ax2.set_xlabel(\"PC1\")\n",
    "        ax2.set_ylabel(\"PC2\")\n",
    "        ax2.set_zlabel(\"PC3\")\n",
    "        ax2.view_init(elev=30, azim=120)\n",
    "        ax2.set_title(f\"Spectral clusters (n={n_clusters})\\nSilhouette={sil_spec:.3f}\")\n",
    "        fig.colorbar(sc2, ax=ax2, pad=0.05, shrink=0.6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Map side-by-side: GMM vs Spectral ---\n",
    "    df_gmm = cluster_df.copy()\n",
    "    df_gmm['cluster'] = labels_gmm\n",
    "\n",
    "    df_spectral = cluster_df.copy()\n",
    "    df_spectral['cluster'] = labels_spectral\n",
    "\n",
    "    fig, (ax1_map, ax2_map) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    plot_cluster_map_germany(df_gmm, germany, ax=ax1_map)\n",
    "    ax1_map.set_title(f'GMM clusters (n={n_clusters})', pad=10, fontsize=12)\n",
    "\n",
    "    plot_cluster_map_germany(df_spectral, germany, ax=ax2_map)\n",
    "    ax2_map.set_title(f'Spectral clusters (n={n_clusters})', pad=10, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e63be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
